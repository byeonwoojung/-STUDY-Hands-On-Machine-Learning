# 2장 : 머신머신러닝 프로젝트 처음부터 끝까지

**2장에서 ‘StatLib 저장소에 있는 캘리포니아 주택 가격 데이터셋’ 사용함**

## 생각 정리

- 모든 수의 소인수 분해는 유일한 것을 이용 → 어떤 것을 유일하게 표현 가능
서로 다른 소수의 최대공약수는 1인 것을 이용 → 서로 다른 소수의 거듭제곱도 유일함
    - 각 특징(특성)을 2, 3, 5 등 각 소수(prime)로 정의하고, 어떤 것이 어떠한 특징(특성)을 가지면
    1에서 그 특징에 해당하는 소수를 곱해주는 아이디어 이용’하면 어떨까..
    (어떤 특징이 강하다면, 그 해당 소수의 거듭 제곱을 곱하도록 설정)
        - 특성마다 소수를 정해준다는 말이 적절한 말인지 고민이 필요
        - 어떤 것을 정해줄 때 이용할 것인지 고민이 필요
    - 어떤 것이 2라는 특성, 3이라는 특성, 7이라는 특성이 모두 1만큼 정도로 가진 것이면
    2*3*7 = 42 라는 값을 갖게 함
    - 어떤 것이 2라는 특성이 3만큼, 3이라는 특성이 2만큼, 7이라는 특성이 1만큼 정도로 가진 것이면
    2^3 * 3^2 * 7^1 = 504 라는 값을 갖게 함
    - 어떤 것들이 관련 있는지 비교할 때는 소인수를 이용함!!

## 머신러닝 프로젝트 주요 단계

- 큰 그림 보기
- 데이터 구하기
- 데이터로부터 통찰을 얻기 위해 탐색하고 시각화하기
- 머신러닝 알고리즘을 위해 데이터를 준비하기
- 모델을 선택하고 훈련시키기
- 모델을 상세히 조정하기
- 솔루션 제시하기
- 시스템을 론칭하고 모니터링하고 유지보수하기’

## 머신러닝 프로젝트 체크리스트 : 부록 B → 열어보기!!

1. 문제를 정의하고 큰 그림을 그림
    - 목표를 비즈니스 용어로 정의함
    - 이 솔루션은 어떻게 사용될 것인가?
    - (만약 있다면) 현재 솔루션이나 차선책은 무엇인가?
    - 어떤 문제라고 정의할 수 있나? (지도/비지도, 온라인/오프라인 등)
    - 성능을 어떻게 측정해야 하나?
    - 성능 지표가 비즈니스 목표에 연결되어 있나?
    - 비즈니스 목표에 도달하기 위해 필요한 최소한의 성능은 얼마인가?
    - 해당 분야의 전문가가 있나?
    - 수동으로 문제를 해결하는 방법은 무엇인가?
    - 세운 가정(가설)을 나열함
    - 가능하면 가정(가설)을 검증함
2. 데이터를 수집함 (새로운 데이터 쉽게 얻을 수 있도록 최대한 자동화하기)
    - 필요한 데이터와 양을 나열함
    - 데이터를 얻을 수 있는 곳을 찾아 기록함
    - 얼마나 많은 공간이 필요한지 확인함
    - 법률상의 의무가 있는지 확인하고 필요하다면 인가를 받음
    - 접근 권한을 획득함
    - 작업 환경을 만듦 (충분한 저장 공간 등)
    - 데이터를 수집함
    - 데이터를 조작하기 편리한 형태로 변환함 (데이터 자체는 바꾸지 않음)
    - 민감한 정보가 삭제되었거나 보호되었는지 검증함 (개인정보 비식별화 등)
    - 데이터의 크기와 타입 (시계열, 표본, 지리정보 등) 확인함
    - 테스트 세트를 샘플링하여 따로 떼어놓고 절대 들여다보지 않음 (데이터 염탐 금지)
3. 데이터를 탐색함 (해당 분야의 전문가에게 조언을 구함)
    - 데이터 탐색을 위해 복사본을 생성함 (필요시 샘플링하여 적절한 크기로 줄임)
    - 데이터 탐색 결과를 저장하기 위해 주피터 노트북을 만듦
    - 각 특성의 특징을 조사함
        - 이름
        - 타입 (범주형, 정수/부동소수, 최댓값/최솟값 유무, 텍스트, 구조적인 문자열 등)
        - 누락된 값의 비율(%)
        - 잡음 정도와 잡음의 종류(확률적, 이상치, 반올림 에러 등)
        - 이 작업에 유용한 정도
        - 분포 형태 (가우시안, 균등, 로그 등)
    - 지도 학습 작업이라면 타깃 속성을 구분함
    - 데이터를 시각화함
    - 특성 간의 상관관계를 조사함
    - 수동으로 문제를 해결할 수 있는 방법을 찾아봄
    - 적용이 가능한 변환을 찾음
    - 추가로 유용한 데이터를 찾음 (있으면 ‘2. 데이터를 수집함’으로 돌아감)
    - 조사한 것을 기록함
4. 데이터를 준비함
    1. 데이터 복사본으로 작업함
    2. 적용한 모든 데이터 변환은 함수로 만듦
        - 다음에 새로운 데이터 얻을 때 데이터 준비를 쉽게 할 수 있음
        - 다음 프로젝트에 이 변환을 쉽게 적용할 수 있음
        - 테스트 세트를 정제하고 변환하기 위해서
        - 솔루션이 서비스에 투입된 후 새로운 데이터 샘플을 정제하고 변환하기 위해서
        - 하이퍼파라미터로 준비 단계를 쉽게 선택하기 위해서
    - 데이터 정제
        - 이상치 수정 or 삭제함 (선택사항)
        - 누락된 값을 채우거나 그 행을 제거함
    - 특성 선택 (선택사항)
        - 작업에 유용하지 않은 정보를 가진 특성을 제거
    - 적절한 특성 공학
        - 연속 특성 이산화하기
        - 특성 분해하기 (ex. 범주형, 날짜/시간 등)
        - 가능한 특성 변환 추가하기 (ex. log(x), sqrt(x), x^2 등)
        - 특성 조합해 가능성 있는 새로운 특성 만들기
    - 특성 스케일 조정
        - 표준화 또는 정규화
5. 가능성 있는 몇 개의 모델을 고름
    1. 데이터 매우 클 때,  여러 가지 모델을 일정 시간 안에 훈련시킬 수 있도록 데이터를 샘플링하여
    작은 훈련 세트를 만드는 것이 좋음
    (but, 규모가 큰 신경망이나 랜덤 포레스트 같은 복잡한 모델은 만들기 어려워짐)
    2. 가능한 한 최대로 이 단계들을 자동화함
    - 여러 종류의 모델을 기본 매개변수를 사용해 신속하게 많이 훈련시킴
    (ex. 선형 모델, 나이브 베이지, SVM, 랜덤 포레스트, 신경망 등)
    - 성능을 측정하고 비교함
        - 각 모델에서 N-겹 교차 검증을 사용해 N개 폴드의 성능에 대한 평균과 표준 편차를 계산함
    - 각 알고리즘에서 가장 두드러진 변수를 분석함
    - 모델이 만드는 에러의 종류를 분석함
        - 이 에러를 피하기 위해 사람이 사용하는 데이터는 무엇인가?
    - 간단한 특성 선택과 특성 공학 단계를 수행함
    - 이전 다섯 단계를 1번이나 2번 빠르게 반복
    - 다른 종류의 에러를 만드는 모델을 중심으로 가장 가능성 높은 모델을 3개에서 5개 정도 추림
6. 시스템을 세밀하게 튜닝함
    1. 가능한 한 많은 데이터 사용하는 것이 좋음 → 세부 튜닝의 마지막 단계로 갈수록 더욱 그러함
    2. 자동화!!!!
    - 교차 검증을 사용해 하이퍼파라미터를 정밀 튜닝함
        - 하이퍼파라미터를 사용해 데이터 변환을 선택함
        특히 확신이 없는 경우 ‘누락된 값을 0으로 채울 것인지, 중간값으로 채울 것인지, 그 행을 버릴 것인지’ 선택함
        - 탐색할 하이퍼파라미터의 값이 매우 적지 않으면 ‘그리드 서치’보다 ‘랜덤 서치’ 사용함
        훈련 시간이 오래 걸리면 ‘베이지안 최적화 방법’ 사용함
        (ex. 가우시안 프로세스 사전 확률 사용)
    - 앙상블 방법을 시도해보기 (최고의 모델들을 연결하면 종종 개별 모델을 실행하는 것보다 좋음)
    - 최종 모델에 확신이 선 후 일반화 오차를 추정하기 위해 테스트 세트에서 성능을 측정함
        - 일반화 오차를 측정한 후에는 모델을 변경하지 말기
        → 변경 시 테스트 세트에 과대적합되기 시작함!!!
7. 솔루션을 출시함
    - 지금까지의 작업을 문서화함
    - 발표 자료 만듦 (먼저 큰 그림을 부각시킴)
    - 이 솔루션이 어떻게 비즈니스의 목표를 달성하는지 설명함
    - 작업 과정에서 알게 된 흥미로운 점들을 잊지 말고 설명함
        - 성공한 것과 그렇지 못한 것을 설명함
        - 우리가 세운 과정과 시스템의 제약을 나열함
    - 그래프나 기억하기 쉬운 문장으로 핵심 내용을 전달하기
    (ex. “중간 소득이 주택 가격에 대한 가장 중요한 예측 변수이다.”)
8. 시스템을 론칭함
    - 서비스에 투입하기 위해 솔루션을 준비함 (실제 입력 데이터 연결, 단위 테스트 작성 등)
    - 시스템의 서비스 성능을 일정한 간격으로 확인하고 성능이 감소했을 때, 알림을 받기 위해 모니터링 코드를 작성함
        - 아주 느리게 감소되는 현상을 주의하기
        데이터가 변화함에 따라 모델이 점차 구식이 되는 경향이 있음
        - 성능 측정에 사람의 개입이 필요할수도 (ex. 크라우드소싱(Crowdsourcing) 서비스 통해)
        - 입력 데이터의 품질도 모니터링함 (ex. 오동작 센서가 무작위한 값을 보내거나, 다른 팀의 출력 품질이 나쁜 경우)/ 특히 온라인 학습 시스템의 경우 중요함 !!
    - 정기적으로 새로운 데이터에서 모델을 다시 훈련시킴 (가능한 한 자동화함!!)
    

## 사이킷런 설계 철학 → 열어보기 !! (데이터 준비 - 데이터 정제 관련)

- 일관성 (모든 객체과 일관되고 단순한 인터페이스 공유함)
    - 추정기 (Estimator)
        - 데이터셋 기반으로 일련의 모델 파라미터들을 추정하는 객체 (ex. imputer 객체)
        - 추정 자체는 fit() 메서드에 의해 수행되고 하나의 매개변수로 하나의 데이터셋만 전달함
        (지도 학습 알고리즘에서는 매개변수 두 개, 두 번째 데이터셋은 레이블을 담고 있음)
        - 추정 과정에서 필요한 다른 매개변수들은 하이퍼파라미터로 간주되고, 인스턴스 변수로 저장됨
        (ex. imputer 객체의 strategy 매개변수 → median으로 설정했었음!!)
        * 인스턴스 변수 : 객체지향 프로그래밍에서 객체가 각각 독립적으로 가지고 있는 변수임
        (보통 생성자의 매개변수로 전달함)
    - 변환기 (Transformer)
        - 데이터셋을 변환하는 추정기 (ex. imputer 객체)
        - API 단순함
            - 변환은 데이터셋을 매개변수로 전달받은 transform() 메서드가 수행하고, 변환된 데이터셋을 반환함
            - 이러한 변환은 일반적으로 imputer와 같은 학습된 모델 파라미터에 의해 결정됨
        - 모든 변환기는 fit()과 transform()을 연달아 호출하는 것과 동일한 fit_transform() 메서드도 가지고 있음 (최적화되어 있어서 더 빠름)
    - 예측기 (Predictor)
        - 일부 추정기는 주어진 데이터셋에 대해 예측을 만들 수 있음
        (ex. LinearRegression 모델이 어떤 나라의 1인당 GDP가 주어질 때 삶의 만족도 예측했었음)
        - predict() 메서드 : 새로운 데이터셋을 받아 이에 상응하는 예측값을 반환함
        - score() 메서드 : 테스트 세트 (지도 학습 알고리즘에서는 레이블도 포함)를 사용해 예측의 품질을 측정함
- 검사 기능
    - 모든 추정기의 하이퍼파라미터는 공개 인스턴스 변수로 직접 접근할 수 있음
    (ex. imputer.strategy)
    - 모든 추정기의 학습된 모델 파라미터도 접미사로 밑줄을 붙여서 공개 인스턴스 변수로 제공됨
    (ex. imputer.statistics_)
- 클래스 남용 방지
    - 데이터셋을 별도의 클래스가 아니라 넘파이 배열이나 사이파이 희소(sparse) 행렬로 표현함
    - 하이퍼파라미터는 보통의 파이썬 문자열이나 숫자임
- 조합성
    - 기존의 구성요소를 최대한 재사용
    - 여러 변환기를 연결한 다음 마지막에 추정기 하나를 배치한 Pipeline 추정기를 쉽게 만들 수도 있음
- 합리적인 기본값

## 실제 데이터로 작업하기

- UC 얼바인 머신러닝 저장소
- 캐글
- 아마존 AWS 데이터셋
- 데이터 포털
- 오픈 데이터 모니터
- 퀀들
- 위키백과 머신러닝 데이터셋 목록
- Quora
- 데이터셋 서브레딧

## 큰 그림 보기

- 캘리포니아 인구조사 데이터 사용해 캘리포니아 주택 가격 모델 만들자.
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/c556da20-87f2-4410-bc6b-6e809d6360f2/image.png)
    
- 이 데이터는 캘리포니아 블록 그룹마다 인구, 중간 소득, 중간 주택 가격 등을 담음
- 블록 그룹은 미국 인구 조사국에서 샘플 데이터를 발표하는 데 사용하는 최소한의 지리적 단위
(한 블록 그룹이 보통 600~3,000명의 인구 / 블록 그룹 = 구역 이라 할 것)
- 데이터로 모델 학습 시켜서 다른 측정 데이터가 주어졌을 때 ‘**구역의 중간 주택 가격 예측**’해야 함

### 문제 정의 (‘주택 가격 모델 만들기’ 예제)

- 질문 : “비즈니스의 목적이 정확히 무엇인가?”
    - 이 모델의 출력(구역의 중간 주택 가격에 대한 예측)이
    여러가지 다른 신호와 함께 다른 머신러닝 시스템에 입력으로 사용되는 것이라 하자.
    (즉, 뒤따르는 시스템은 해당 지역에 투자할 가치가 있는지 결정하는 것임)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/cc124c9f-135b-4c0c-b2f1-4655a02ca5de/image.png)
        
    - 파이프라인 : 데이터 처리 컴포넌트(Component)들이 연속되어 있는 것
        - 보통 컴포넌트들은 비동기적으로 동작
        - 각 컴포넌트는 많은 데이터를 추출해 처리하고 그 결과를 다른 데이터 저장소로 보냄
        → 일정 시간 후 파이프라인의 다음 컴포넌트가 그 데이터를 추출해 자신의 출력 결과 만듦
        - 각 컴포넌트는 완전히 독립적임 (컴포넌트 사이의 인터페이스는 데이터 저장소 뿐)
            - 이러한 데이터 흐름도 덕분에 시스템을 이해하기 쉽게 만들고, 각 팀은 각자의 컴포넌트에
            집중할 수 있음
            - 한 컴포넌트가 다운되더라도, 하위 컴포넌트는 문제가 생긴 컴포넌트의 마지막 출력을 사용해
            적어도 한동안은 평상시와 같이 계속 동작 가능
            - BUT, 모니터링 적절히 되지 않으면 고장 난 컴포넌트 모를 수도 있음
- 질문 : “현재 솔루션은 어떻게 구성되어 있나?”
    - 현재 솔루션이 있다면 ‘정보와 참고 성능으로 사용’ 가능
    - 이전 솔루션을 보았을 때, 현재 모델을 훈련시키는 것이 유용하다고 판단하자.
    (구역의 데이터를 기반으로 중간 주택 가격을 예측하는 모델)
    → 이제 모델을 훈련시키기 위한 준비를 할 것임!!
- 문제 정의
    - 레이블을 포함하기 때문에 ‘지도 학습 작업’임
    - 구역의 인구, 중간 소득 등 특성이 여러 개이기 때문에 ‘다중 회귀 문제’임
    - 각 구역마다 하나의 값을 예측하기 때문에 ‘단병량 회귀 문제’임
    - 데이터의 연속적인 흐름이고, 변하는 데이터 적응 필요성이 없고, 데이터가 메모리에 들어갈 만큼
    충분하기 때문에 ’배치 학습’이 적절함
    * 데이터가 너무 크면, 맵리듀스(MapReduce) 기술을 사용해 배치 학습을 여러 서버로 분할하거나,
      온라인 학습 기법을 사용할 수 있음

### 성능 측정 지표 선택

- **평균 제곱근 오차(Root Mean Square Error, RMSE)** : 회귀 문제의 전형적인 성능 지표
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/359d428e-e270-4d4a-9680-1a9d1f46e778/image.png)
    
    - 유클리드 노름(일반적인 직선 상의 거리)
    - **‘제곱항의 합의 제곱근’ → 작은 오차를 크게 만들 수 있음 → 이상치에 더 민감할 것**
    - **본 예제에서는 이상치가 많다고 가정하여 ‘평균 제곱근 오차(RMSE)’를 이용한다고 하자!**
    - 용어
        - m : RMSE를 측정할 데이터셋에 있는 샘플 수
        (ex. 2,000개 구역의 검증 세트 → m = 2,000)
        - x^(i) : 데이터셋에 있는 i번째 샘플(레이블은 제외)의 전체 특성값의 벡터
        y^(i) : 해당 레이블 (해당 샘플의 기대 출력값)
        (ex. 데이터셋에 있는 첫 번째 구역이 경도 -118.29˚, 위도 33.91˚에 위치, 중간 소득이 $38.372 이고, 주민이 1,416명, 중간 주택 가격이 $156,400 일 때,)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/1d0bcc4d-bd93-410a-a146-a0f57397eaa6/image.png)
            
        - X : 데이터셋에 있는 모든 샘플의 모든 특성값(레이블은 제외)을 포함하는 행렬
        샘플 하나가 하나의 행이어서 i번째 행은 x^(i)의 전치 (= (x^(i))^T)임
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/63ebf991-64cd-4b82-89db-6ce3417eda4b/image.png)
            
        - h : 시스템의 예측 함수 (= 가설)
            - 시스템이 하나의 샘플 특성 벡터 x^(i)를 받으면
            그 샘플에 대한 예측값 y^(i)의 햇 = h(x^(i))을 출력함
            - 시스템 첫 번째 구역의 중간 주택 가격을 $158,400이라고 예측한다면
            y^(i)의 햇 = h(x^(i)) = 158,400 이고
            이 구역에 대한 예측 오차 = y^(i)의 햇 - y^(i) = 2,000 이다.
        - RSME(X, h) : 가설 h를 사용하여 일련의 샘플을 평가하는 비용 함수
    
- **평균 절대 오차(Mean Absolute Error) or 평균 절대 편차(Mean Absolute Deviation)**
    - **(중요) 이상치로 보이는 구역이 많을 때 이용 → 오차가 큰 것이 많을 것!!**
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/83d2f702-aa84-4003-a762-5b98b6d27cd9/image.png)
        
    - 맨해튼 노름 (직각으로 된 구획의 거리)
    - **절댓값(노름)의 합
    → 제곱항을 이용하는 것보다 오차를 상대적으로 더 작게 볼 수 있음
    → 이상치에 덜 민감하고, 이상치가 드물게 나타나면 RMSE과 비슷할 것**

### 가정 검사

- 가정을 나열하고 검사하기
- 정확한 가격을 구하는 것보다 (저렴, 보통, 고가) 라는 카테고리만으로도 충분할 수도 있음
→ 회귀 시스템이 아닌, 분류 작업일 수 있음 → 가정을 나열하고 검사해보는 것이 필요함!
- **하지만 본 예제에서는 ‘정확한 실제 가격’이 필요하다고 가정하자.**

## 데이터 가져오기

### 작업 환경 만들기

- 주피터, 넘파이, 판다스, 맷플로립, 사이킷런 파이썬 패키지 설치

### 데이터 다운로드

- housing.tgz 파일을 내려받고, tar xzf housing.tgz 명령으로 csv 파일로 압축을 풀 수 있음
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/fb658163-81dc-4d9a-b24e-0bda3242f2b9/image.png)
    
- 데이터 내려받는 자동화 방법
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/7c4f3801-ea5c-4de3-9b48-323946172c97/image.png)
    
    - fetch_housing_data() 함수 호출
    → 현재 작업공간에 datasets/housing 디렉터리 만듦
    → housing.tgz 파일을 내려받음
    → housing.tgz 파일을 같은 디렉터리에 압축을 풀어 housing.csv 파일을 만듦
- 판다스 이용하여 데이터 읽기
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/1ae72a87-dae7-4894-9b2c-926f93b3f694/image.png)
    
    - 모든 데이터를 담은 판다스의 데이터프레임 객체를 반환하는 함수 정의

### 데이터 구조 훑어보기

- (매우 중요) 데이터를 훑어볼 때 테스트 세트를 먼저 떼어 놓고, 절대 들여다보지 않기!!
→ 새로운 데이터에 좋지 못한 성능을 내는 머신러닝 모델 만들 수 있음!!

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/6b131a6e-3ed7-480c-933f-4c61fabfaadd/image.png)

- load_housing_data() 함수를 호출하여
모든 데이터를 담은 판다스의 데이터프레임 객체를 반환하여 housing에 저장

---

- head() 메서드 : 데이터프레임의 상위 5개 행을 출력함 → 데이터의 형태 파악하는 것임!!
    - longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity 등
    10개의 특성이 이는 걸로 파악됨
- info() 메서드
 : 데이터에 대한 간략한 설명 확인 (전체 행 수, 각 특성의 데이터 타입과 NULL이 아닌 값의 개수 등)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/503333f8-74ca-430f-a49c-2ac14ab410ef/image.png)
    
    - 0부터 20639까지 총 20640의 샘플 존재
    - 0부터 9까지 총 10개의 컬럼(특성) 존재
    - total_bedrooms 특성만 20433개의 NULL이 아닌 값 존재 → 207개 전처리 필요
    (나머지 특성은 20640개의 NULL이 아닌 값 존재)
    - ocean_proximity 특성만 객체형 타입이고, 나머지는 숫자(부동소수)형 타입

---

- value_counts() 메서드 : 범주형 특성을 탐색함
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/209860f3-f017-45bf-863f-024ae232cbc0/image.png)
    
    - 어떤 카테고리 존재하는지, 각 카테고리마다 얼마나 많은 구역 있는지 확인 가능
    - ocean_proximity 특성은 특히 ‘범주형 타입’임

- describe() 메서드 : 숫자형 특성을 탐색함
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/810e8a18-b89e-41db-b36c-92955ed6ac74/image.png)
    
    - count(개수), mean(평균), std(표준편차), min(최솟값),
    n%(백분위수 / n=25, 50, 75일 때 각각 제1, 2, 3사분위수 / 제2사분위수 = 중간값)
    - 표로 데이터 형태를 파악하기 어렵기 때문에 ‘히스토그램’ 이용!!!

---

- pandas의 hist() 메서드 호출 : 숫자형 특성에 대한 히스토그램 출력
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/edb90a5f-1088-46e4-99cb-74e2ad809f04/image.png)
    
    - 주어진 값의 범위(수평축), 샘플수(수직축)
    - **데이터 형태 파악 조금 어려움** → 그래프 설정 필요해보임 → **matplotlib 이용해보자**

- matplotlib 이용하여 히스토그램 출력하기
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/ffdbdf8d-27b1-4f1f-9adc-d422299cf223/image.png)
    
    - %matplotlib inline
        - Jupyter Notebook에서 그래프를 인라인(즉, 노트북 안에)으로 표시하기 위한 매직 명령어
        - 여기서는 Jupyter Notebook에서 matplotlib을 이용한 시각화를 Notebook 내에서 출력하도록 설정해 주는 매직 커맨드임
            - 이 명령어는 Jupyter Notebook에서 matplotlib 플롯이 별도의 창이 아니라 노트북 셀 안에 출력되도록 만듦
            - 이 명령어를 사용하지 않아도 pandas.hist()는 제대로 작동하지만, 그래프가 노트북 셀 안에 나타나지 않을 수 있음
    - hist() 메서드
        - housing.hist(bins=50, figsize=(20,15))
            - bins=50 : 그래프 구간의 크기는 50
            - figsize=(20,15) : 그래프 가로 길이 20, 세로 길이 15
        - pandas.DataFrame 객체 housing에 포함된 .hist() 메서드임
        (판다스 데이터프레임인 객체 housing)
        - housing 데이터프레임의 각 숫자형 열에 대해서만 히스토그램을 생성함
        - 내부적으로 matplotlib을 사용해 히스토그램을 그림
    - show() 메서드
        - plt.show() : 그래프를 그려줌
        → 쓰지 않아도 주피터에서는 자동으로 그려주는데, 명시적으로 표현하기 위해 작성
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/bc615c7a-771a-451c-930c-5fd4f12080fb/image.png)
            
            - median_income : US달러 표현되어 있지 않고 스케일을 조정한 것 (전처리)
             ex. 3은 약 30,000달러임
            - housing_median_age : 최댓값과 최솟값 한정함
            - housing_house_value : 최댓값과 최솟값 한정함
            → 타깃 속성(레이블) 사용되기 때문에 문제가 될 수 있음
            → 정확한 예측값이 필요한지, 상한을 정해도 되는 것인지 논의 필요
            → 상한을 넘어가도 정확한 예측값 필요할 때는 한곗값 밖의 구역에 대한 정확한 레이블을
                구함 or 훈련 세트와 테스트 세트에서 이러한 구역 제거하는 방법도 있음
            - 특성들의 스케일이 서로 많이 다름 → ‘스케일링’ 진행함
            - 히스토그램 꼬리가 두꺼운 경우가 많아서 머신러닝 알고리즘 패턴 찾기 어려운 경우 많음
            → 분포를 변형함!

### 테스트 세트 만들기

- 무작위로 어떤 샘플을 선택해서 데이터셋의 20% 정도 (데이터셋이 매우 크면 그보다 적게) 떼어 놓음
- 데이터 스누핑(Data Snooping) 편향
    - ‘3. 데이터 훑어보기’ 전에 테스트 세트를 미리 분리해놓고, 절대 들여다보면 안 됨
    - 테스트 세트를 들여다보면, 테스트 세트에서 드러나는 어떤 패턴에 속아 특정 머신러닝 모델을 선택함
    - 테스트 세트로 일반화 오차를 추정하면 매우 낙관적인 추정이 되어 시스템 론칭했을 때, 기대한 성능이
    나오지 않을 것임!!

- 무작위 샘플링 진행
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/19846724-ccf5-43f6-894a-7ce6ddaab55e/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/87cdb89a-b355-45b3-a6b6-bc6db163a45d/ea89b392-70e0-484c-96d7-1b1fcd7ab510.png)
    
    - split_train_test 함수 정의
        - **np.random.permutation 함수**로 **데이터의 인덱스를 무작위로 섞어 배열을 반환하여** shuffled_indices에 저장
        - test_set_size는 (전체 데이터의 길이 * 전체 데이터 세트에서 test 데이터의 비율)
        - 무작위로 섞인 shuffled_indices에서 테스트 세트의 사이즈만큼 앞 부분부터 떼어내어 train_indices에 저장 (무작위로 섞인 ‘**데이터의 인덱스**’가 저장되어 있음!!)
        - iloc 메서드로 데이터를 나눠진 인덱스에 따라 훈련 세트와 테스트 세트로 분리하여 반환
            - 데이터프레임.iloc[행 인덱스, 열 인덱스]
            - 데이터프레임.iloc[인덱스1 : 인덱스2] // 인덱스 1부터 인덱스 2 전까지
            - 데이터프레임.iloc[특정 행들의 인덱스]
    - np.random.seed(42)
    np.random.permutation()
        - 시드 42에서 난수 생성기를 시작하도록 초기화한 후에 난수를 생성하면
        나중에 같은 시드 42에서 시작하도록 초기화하고 난수 생성했을 때 동일한 결과 나오게 됨!!
        → 테스트 세트가 난수 생성 때마다 변화하지 않도록 함!!
        - (중요!!) 안정적으로 훈련/테스트 세트를 분할할 수 있지만 **데이터셋을 업데이트할 때 문제 생김
        → 아래의 2가지 방법 (식별자, 사이킷런 무작위 샘플링 함수) 이용**

- (참고) 난수 생성 관련
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/141b1928-63b8-40df-9bda-b523ec3a5355/image.png)
    
    - np.random.seed(숫자)
    np.random.rand(개수)
        - 난수 생성기를 ‘**시드 숫자’에서 시작**하도록 초기화
        - 난수 생성기를 ‘**초기 상태(시드 숫자)**’에서 첫 번째 난수 세트를 개수만큼 생성함
        - 이후에 같은 숫자의 시드에서 시작하도록 np.random.seed(숫자), np.random.rand(개수) 쓰면 알고리즘 자체가 똑같은 초기 상태에서 시작하기 때문에 같은 난수를 생성함!!
    - ‘np.random.seed(숫자) 없이’
     np.random.rand(개수) 만 호출 시
        - 시드 없이 난수를 생성하면 운영 체제에서 제공하는 **진짜 난수(엔트로피) 기반**으로 초기화
        → 다른 결과가 나옴

- **(중요) 데이터셋 업데이트에도 안정적인 훈련/테스트 분할하는 방법**
    - **샘플의 식별자** 이용
        - ‘**샘플의 식별자**’ 사용하여 테스트 세트로 보낼지 결정함
         (ex. 각 샘플마다 식별자의 해시값 계산하여 해시 최댓값의 20%보다 작거나 같은 샘플만
          테스트 세트로 보냄)
        → 여러 번 반복 실행되면서 데이터셋이 갱신되더라도 테스트 세트가 동일하게 유지됨
        → 새로운 테스트 세트는 새 샘플의 20%를 갖지만 이전에 훈련 세트에 있던 샘플은 포함 안 함
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/279d90d0-4e5f-49ec-b46f-ec03ae39bc56/image.png)
            
            - (이해 필요) split_train_test_by_id() 함수
                - 식별자의 체크섬을 기준으로 체크섬 값이 일정 백분율 이하인 샘플을 테스트 세트에 저장
                - 체크섬(checksum): 파일 전송 과정에서 발생할 수 있는 손실 여부를 판단하기 위해 사용되는 값으로 파일이 달라지면 체크섬 값이 달라짐
                - 참고: [위키백과: 체크섬](https://ko.wikipedia.org/wiki/%EC%B2%B4%ED%81%AC%EC%84%AC)
            - (이해 필요) zlib.crc32() 함수 : 파일의 체크섬을 CRC 방식으로 계산한 32비트 정수 반환
                - CRC(순환 중복 검사): 파일의 체크섬을 계산하는 방식
                - 0xffffffff : 32비트 정수 중에서 가장 큰 정수, 즉 2**32 - 1
                - test_ratio * 2**32 : 32비트 정수 중에서 test_ratio 비율에 해당하는 정수 예를 들어, test_ratio = 0.2 이며, 하위 20%에 해당하는 정수
            - & : **이진 논리곱**(binary AND)이라는 비트 연산자
                - 이진법으로 표현된 두 숫자의 논리곱 연산자
                - 동일한 위치의 수가 둘 모두 1일 때만 1로 계산됨
                - 여기서는 0xffffffff와의 비트 연산을 통해 2**32 보다 작은 값으로 제한하기 위해 사용됨하지만 zlib.crc32() 함수가 32비트 정수를 반환하기에 굳이 사용할 필요 없음
        - 고유 식별자 생성 방법 1 : 행의 인덱스를 추가로 하여 식별자로 사용
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/ed0fda4a-f392-41a3-bf5f-315a6759ea9b/image.png)
            
        - 고유 식별자 생성 방법 2 : 경도와 위도를 묶어서 id를 만들어 식별자로 사용
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/40cb8cf3-96da-4c4c-838f-995dd7c1f325/image.png)
            
        - 방법 1, 방법 2 실행했을 때 결과 (index, id 생김)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/0e163eff-767f-4ba4-a69c-46aa8f5db02c/image.png)
            
    
    - **사이킷런의 무작위 샘플링 함수(train_test_split 함수) 이용**
        - 데이터셋을 여러 서브셋으로 나누는 다양한 방법 제공
        - 사이킷런에서 제공하는 train_test_split 함수
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/3d357c93-0757-488c-aa19-7abac2142798/image.png)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/30b165bd-2954-4915-86ec-444c81dec8fb/image.png)
            
            - 난수 생성할 때 정의했던 split_train_test 함수와 비슷하지만 2가지 특징 더 있음
                - ‘random_state 매개변수’ → 난수 초깃값을 지정할 수 있음
                 ex. train_test_split(housing, test_size=0.2, random_state = 42)
                - 행의 개수가 같은 여러 개의 데이터셋을 넘겨서 같은 인덱스를 기반으로 나눌 수 있음
                (데이터프레임이 레이블에 따라 여러 개로 나뉘어 있을 때 유용)
                
- **(중요!!) 계층적 샘플링 (Stratified Sampling)**
    - 샘플을 추출할 때 **전체를 대표할 수 있도록 계층(그룹)의 비율을 유지**하면서 추출해야 함
    → StratifiedShuffleSplit() 함수 이용할 것
    - (예시) 미국 인구의 남녀 비율이 51.3%, 48.7%라면 이 비율을 유지하면서 샘플을 추출
    → 샘플 1,000명, 샘플이 남자일 확률 51.8%
    → 1000*0.513 ≥ 10이고 1000*0.518*0.487 ≥ 10인 이항분포
    → 평균이 1000*51.3=513, 표준편차 sqrt(1000*0.518*0.487)=15.8인 정규분포로 근사 가능
    → 위의 정규분포에서 490 이하와 540이상인 부분의 면적은 11.65%(약 12%) 됨
    → 완전한 무작위 샘플링을 통해 설문 진행했을 때, 49%보다 적거나 54%보다 많은 여성이 테스트 세트에 들어갈 확률은 약 12%정도 됨
    → 어느 방법이든 설문조사 결과를 크게 편향시킴
    - 중간 소득(median_income)이 중간 주택 가격을 예측하는 데 중요하다고 하자.
        - 테스트 세트가 전체 데이터셋에 있는 여러 소득 카테고리를 잘 대표하도록 ‘**계층적 샘플링**’ 필요
        - 중간 소득이 연속적인 숫자형 특성이므로 **소득에 대한 카테고리 특성** 만들어야 함
        → pd.cut() 함수 사용할 것
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/6163898c-cc43-4224-8bda-92146977da45/image.png)
            
            → 중간소득 대부분 1.5 ~ 6($15,000 ~ $60,000) 사이에 모여 있고, 6($60,000) 넘기도 함
            → 계층별로 데이터셋에 충분한 샘플 수가 있도록 잘 나눠야 함
            
        
    - pd.cut() 함수
        - 판다스(Pandas) 라이브러리에서 데이터를 여러 구간(bins)으로 나누는 데 사용
        (값의 분포를 요약하거나 그룹화할 때 사용)
        - 연속형 데이터를 이산형 데이터로 변환함
        - pd.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise')
            - x : 나눌 데이터. 배열, 시리즈, 또는 데이터프레임의 열
            - bins : 데이터의 구간 경계값을 지정하거나 구간 수를 지정함
                - 정수 : 균등한 크기의 구간 수
                - 배열 : 구간 경계값을 직접 지정함
            - right : 각 구간의 오른쪽 경계를 포함할지 여부 (기본값은 True : 오른쪽 포함)
            - labels : 구간에 대해 사용할 레이블 (기본값은 각 구간의 문자열 표현)
            - retbins : True로 설정하면 구간 경계값도 반환
            - precision : 경계값의 소수점 자리수
            - include_lowest : 가장 낮은 경계값을 포함할지 여부
            - duplicates : 중복된 구간이 있을 경우 처리 방법
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/8b876f3d-e12c-4f42-a9f5-ba5b6fa56c2e/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/4ee579de-5911-4178-b54b-3e60308348ff/image.png)
        
        - 카테고리 5개를 가진 소득 카테고리 특성 만듦 (1부터 5까지 레이블 가짐)
        - 카테고리 1 : 0에서 1.5까지($15,000이하)
        - 카테고리 2: 1.5에서 3까지 … 등등
    
    - StratifiedShuffleSplit() 함수
        - 사이킷런 라이브러리에서 제공하는 교차 검증(Cross-Validation) 또는 데이터 분할 도구
        - 데이터셋을 **무작위로 섞어** 훈련 세트와 테스트 세트로 나누면서 **각 클래스의 비율(분포) 유지**
        - 클래스 불균형이 있는 데이터셋에서 유용함
        (어떤 클래스의 샘플이 매우 적은 경우, 단순히 데이터를 랜덤하게 나누면 그 클래스가 훈련 또는 테스트 세트에서 빠질 수 있는데 이를 방지함)
        - ‘소득 카테고리 만들어 훈련/테스트 세트를 분류한 것(계층적 샘플링)’에서 테스트 세트의 소득 카테고리의 비율과 ‘전체 데이터’에서 소득 카테고리 비율과 유사함 !!
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/b45912f6-ddc3-4f4d-bc0c-976bf9129734/image.png)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/1373a410-7457-4881-8579-f55d47544c31/image.png)
            
        - 자세히 살펴보자.
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/e20c32fd-62ef-488b-b5dc-2a72f38a8e6d/image.png)
            
            - overal(전체), Stratified(계층 샘플링), Random(무작위 샘플링), Rand. %error(무작위 샘플링 오류율, Strat. %error(계층 샘플링 오류율)
            - ‘전체 데이터에서 소득 카테고리 비율’과 ‘계층적 샘플링한 것에서 소득 카테고리 비율’은
            거의 차이가 없음!! → 샘플링 잘 되어 있다는 의미임!!
    
    - income_cat 특성 삭제하여 데이터 원래 상태 되돌리기
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/47a81230-b121-467a-b6e6-53f05ae8bf37/image.png)
        
        - axis=1 : 열 삭제, axis=0 : 행 삭제(기본값)
        
        → 이제 본격적으로 학습 시작할 것
        

## 데이터 이해를 위한 탐색과 시각화

- 테스트 세트를 분리했는지 먼저 확인
- 훈련 세트 너무 크면 탐색을 위한 별도의 샘플링 필요
(본 예제에서는 훈련 세트 크기가 작으므로 훈련 세트 전체를 탐색할 때 이용)
- 훈련 세트 손상시키지 않기 위해 복사본 만들기
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/dbff1db9-ae11-4115-a3b0-c54ed3355c9f/image.png)
    
- 데이터를 먼저 이해할 필요가 있기 때문에 시각화 해봐야 함

### 지리적 데이터 시각화

- 위도와 경도 → 모든 구역을 산점도로 만들어 데이터 시각화
- 인구 밀집도 나타내기
    - housing.plot(kind=”scatter”, x=”longitude”, y=”latitude”)
        - 산점도로 x축은 경도, y축은 위도로 그래프를 나타내기
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/643456ca-94ac-4c29-a7f9-4981ad983b74/image.png)
        
        → 캘리포니아 지역을 잘 보여주지만 데이터의 특별한 패턴을 찾기 어려움
        
    - housing.plot(kind=”scatter”, x=”longitude”, y=”latitude”, alpha=0.1)
        - alpha를 추가하여 산점도의 투명도 조절함
        - 구역이 밀집되어 있는 곳에 투명도 낮춰 더 진하게 표현하고,
        밀집되어 있지 않은 곳은 투명도 높여 연하게 표현함
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/13807f23-be92-4d4f-a984-091ff99921de/image.png)
        
        → Bay Area, Los Angeles 근처, San Diego 같이 밀집된 지역 눈에 잘 띄고, Sacramento와 Fresno 근처 따라 밀집된 지역이 긴 띠를 이루는 것을 확인 가능
        

- 주택 가격 나타내기
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/5c001870-2199-4f66-a03c-cf6c58330c86/image.png)
    
    - 원의 반지름(s) : 구역의 인구
    - 색상(c) : 가격
    - 컬러 맵은 파란색(낮은 가격) - 빨간색(높은 가격)까지 범위 가지는 jet 사용
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/605769dc-789f-4b94-9bb9-b6d3b75837fa/image.png)
    
    → 주택 가격은 ‘지역(보통, 해안에 근접한 곳에 주택가격 높음)’과 ‘인구 밀도’에 관련이 커 보임
    → 군집 알고리즘(Clustering Algorithm)을 사용해 주요 군집을 찾고 군집의 중심까지의 거래를 재는 특성 추가 가능함
    → 이때, 해안 근접성 특성을 이용하면 좋을 듯 하지만, 북부 캘리포니아 지역의 해안가에서는 주택 가격이 그리 높지 않아 간단하게 규칙 적용하기 어려움!!
    

- 캘리포니아 지도와 합쳐서 표현도 가능함
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/a7a64d20-0eb5-4ff4-a420-2c77b0f6eed1/e4db1803-9f7f-4013-ad0f-1bea21d9c679.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/494170e6-e832-44d2-af98-fff5d10158ba/image.png)
    

## 상관관계 조사

- 데이터셋이 너무 크지 않으므로 모든 특성 간의 **‘표준 상관계수 (피어슨의 r)’를 계산 (**corr() 메서드)
- **표준 상관계수 (피어슨 r)**
    - -1 ≤ r ≤ 1
    - 1에 가까울 때, 강한 양의 상관관계
    - -1에 가까울 때, 강한 음의 상관관계
    - 0에 가까울 때, 선형적인 상관관계 없음
    - 비선형적인 관계는 알 수 없음 (이차 함수, 삼차 함수 등등)
    - 상관계수 그래프 형태
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/f439df41-c409-46e8-a2ee-a78ee57be0d1/image.png)
        
        - 마지막 줄 그래프는 두 축이 완전히 독립적이지 않음에도 상관 계수 0임
        → 비선형 관계의 예임

1. 중간 주택 가격과 다른 특성 사이의 상관관계 크기 확인
    - **corr() 메서드** : 특성 간의 상관계수를 반환
        - **housing에 범주형 데이터가 있으므로 ‘숫자형 데이터만 선택’해서 corr() 메서드 호출**
        (**범주형 데이터를 숫자형 데이터로 바꿔서 진행하기도 함**!!)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/d6b84495-a5e5-450e-b34c-0ef05362fd37/image.png)
            
    - **scatter_matrix() 메서드** : 상관계수 그래프로 확인
        - median_house_value, median_income, total_rooms, housing_median_age
        특성 간의 상관관계 파악함
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/64a9c8df-a535-4c88-ac9a-7d43df4f31e7/image.png)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/73401398-25bd-40cb-93a9-5a871c1c1ad4/image.png)
            
            → **‘중간 주택 가격(median_house_value)’와 ‘중간 소득(median_income)’** 사이에
                **양의 상관관계** 가지는 것처럼 보이고, 나머지는 선형적인 관계가 없어 보임
            → 중간 주택 가격(median_house_value)을 예측하는 데 ‘**중간 소득(median_income)**’이
                가장 유용할 것 같음!
            
        - 중간 소득(median_income) 대 중간 주택 가격(median_house_value) 산점도 확대해 봄
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/17c5ea8e-1971-4df0-b735-c6c172838b9b/image.png)
            
            - 상관관계 매우 강하고, 포인트들이 너무 멀리 퍼져 있지 않음
            - **(중요) 앞서 본 가격 제한값이 $500,000에서 수평선으로 잘 보임 (직선에 가까운 형태로 보임)
            또한, $450,000, $350,000, $280,000 근처에 수평선 보임 (그 아래에도 존재)
            → 이상한 형태 학습하지 않도록 해당 구역을 제거하는 것이 좋을 듯 (데이터 정제 필요)**

## 특성 조합으로 실험

- 산점도 그래프에서 데이터 정제할 것 필요했음 (수평선)
- 특성 사이에서 양의 상관관계를 가지는 것 발견 (중간 소득 대 중간 주택 가격)
(특히, 중간 주택 가격이라는 타깃 속성과 양의 상관관계를 가짐!!)
- 굳이 유용하지 않은 특성들은 조합해버림
(ex. 가구당 방 개수가 중요하지, 가구 수를 모르는 상태에서 특정 구역의 방 개수는 중요하지 않음
차라리 방 개수와 비교하는 것이 좋을 수 있음 등등)

1. 특성 조합
    - **‘특성들을 더 유용하도록 조**합’하면서 **반복적인 탐색**을 통해 빠른 프로토타이핑을 하고,
    더 많은 통찰을 얻는 것이 중요함
    → 타깃 속성과 상관관계가 높을 수 있도록 조합하는 것이 관건 !!
    - 가구당 방의 개수 = total_rooms / households
    방당 침대 개수 = total_bedrooms / total_rooms
    가구당 인원 수 = population / households
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/9df04305-f358-4c7b-800d-9c7b9de8daaa/image.png)
        
    - 조합한 특성을 포함해 ‘상관관계 다시 계산’한 후에 확인하기
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/8c992784-d02b-49f4-93ab-6f709245de0f/image.png)
        
        - ‘방당 침대 개수’는 전체 방 개수나 침대 개수보다 중간 주택 가격과의 상관관계가 훨씬 높음
            - 방당 침대 개수 특성이 더 유용한 특성이 될 것
            - 방당 침대 개수 낮은 집이 더 비싼 경향임
        - ‘가구당 방의 개수’는 중간 주택 가격과 여전히 선형 관계를 가지지 않은 것으로 보임
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/9b9312a6-186e-46a5-8338-24ccf2692c45/image.png)
            
    - housing 의 숫자형 특성 다시 확인
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/975bde9a-92dc-46ab-ae1b-df517c096a53/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/3bdfe825-44e6-48c6-bf81-d8890c589d0a/image.png)
        

## 머신러닝 알고리즘을 위한 데이터 준비

- 머신러닝 알고리즘을 위해 데이터 준비 → 함수를 만들어 ‘**자동화**’해야 함
    - 어떤 데이터셋에 대해서도 데이터 변환 손쉽게 반복할 수 있음
    - 향후 프로젝트에 사용할 수 있는 변환 라이브러리를 점진적으로 구축하게 됨
    - 실제 시스템에서 알고리즘에 새 데이터를 주입하기 전에 변환시키는 데 이 함수 사용할 수 있음
    - 여러 가지 데이터 변환을 쉽게 시도해볼 수 있고 어떤 조합이 가장 좋은지 확인하는 데 편리함

- **먼저, 원래 훈련 세트로 복원함**
    - strat_train_set 다시 복사할 것임
    (조합했던 특성들도 삭제되는 등 이전 상태로 다시 돌아옴)
- **다음으로, 예측 변수와 타깃값에 같은 변형을 적용하지 않도록 ‘예측 변수와 레이블 분리함’**
    - drop() 메서드는 데이터 복사본을 만들고 원래의 strat_train_set에 영향 주지 않음

### 데이터 정제 (Data Cleaning) - 수치형 데이터 위주로

- 누락치가 있는 데이터를 확인
    - isnull() 메서드
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/eb7701ce-0728-48d9-a23b-851ac0f81e9c/image.png)
    

- total_bedrooms 특성에 값이 없는 것 처리
    - 방법 1 : ‘해당 구역 제거’
        - dropna() 메서드
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/b8033092-4e47-4aa9-a2b4-b7c32a157f38/image.png)
            
    - 방법 2 : ‘전체 특성을 삭제’
        - drop() 메서드
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/4d7425a4-8dce-450b-a073-17cd018a710c/image.png)
            
    - 방법 3 : ‘평균, 중간값 등으로 채움’ (본 예제에서 방법3. 중간값으로 채우기 선택)
        - fillna() 메서드 보다는 사이킷런에서 제공하는 SimpleImputer 변환기 사용하는 것이 좋음!!
        - fillna() 메서드
            - 중간값으로 채운다고 하자.
            → 훈련 세트에서 중간값 계산하고, 누락된 값을 이 값으로 채워야 함!!
            → 계산한 중간값 저장하는 것 잊지 말기
            → 나중에 시스템 평가할 때 테스트 세트에 있는 누락된 값과 시스템이 실제 운영될 때
                새로운 데이터에 있는 누락된 값을 채워넣는 데 필요함
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/8e26f307-0f0f-48d3-b68f-17572fed4a99/image.png)
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/707cd710-ed13-4ace-b5c4-86851ceca40f/image.png)
                
                → total_bedrooms 특성의 값 중에 결측치가 중앙값으로 채워짐
                
                → 사이킷런의 SimpleImputer() 메서드로 쉽게 누락된 값 다룰 수 있음
                
        - SimpleImputer() 변환기
            - 사이킷런의 클래스를 이용해 누락치를 쉽게 채울 수 있음
            - **누락치를 특성의 중간값으로 대체한다고 지정하면서 SimpleImputer 객체** 생성
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/31183720-cc8a-4b0b-b625-f40c4611014e/image.png)
                
                - **(중요!!) 수치형인 모든 특성에 대해 imputer를 이용해 중간값을 계산할 것임!!**
            
            - 중간값은 수치형 데이터에서만 계산될 수 있으므로
            ‘**텍스트 특성인 ocean_proximity를 제외’**한 데이터 복사본 housing_num**을 생성함**
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/bff364ef-6296-4914-b917-33a0e5a0545b/image.png)
                
            
            - imputer 객체의 ‘**fit() 메서드**’를 사용해 훈련 데이터에 적용
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/55427f59-73fd-488e-b25a-fca6a3150b4e/image.png)
                
            
            - **‘각 특성의 중간값을 계산’**하여 그 결과를 **‘객체의 statistics_ 속성에 저장’**함
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/a2c0c8ef-b0bb-47b7-a0cd-baa3414b93cd/image.png)
                
                - total_bedrooms 특성에만 누락된 값이 있지만
                나중에 시스템이 서비스될 때 새로운 데이터에서 어떤 값이 누락될지 모르기 때문에
                모든 수치형 특성에 imputer를 적용하는 것이 바람직함!!
            
            - ‘**transform() 메서드**’ 이용해 ‘변형된 특성들이 들어 있는 넘파이 배열 저장’ 한 후에
            ‘**pd.DataFrame() 메서드**’ 이용해 다시 ‘판다스 데이터프레임으로 되돌려서 저장’
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/7653a65d-a043-412d-ba54-ce6e504d9432/image.png)
                
            
            - 누락치를 채운 방법과 누락치가 채워진 복사본 housing_tr 확인
                
                ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/6e7204d6-d290-4d96-9065-9f654636e03d/image.png)
                

### 텍스트와 범주형 특성 다루기 - 텍스트 특성인 ocean_proximity 를 다뤄보기

- ocean_proximity 특성의 확인하기 (housing_cat에 따로 저장)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/783bb4bd-4a41-4f44-aa77-d74f22c4829c/image.png)
    
    → 이 특성 범주형 특성인데, 텍스트인 특성 값들을 숫자로 변환하여 다룰 것
    

- OrdinalEncoder 클래스
    - 범주형 데이터를 숫자로 변환하는 데 사용되는 scikit-learn의 클래스
    - 순서가 있는 범주형 데이터에서 처리할 때 유용함
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/118ec326-f1ae-41c5-9827-26e8d7edc5f6/image.png)
        

- categories_ 인스턴스 변수 : 카테고리 목록 얻음
    - 범주형 특성마다 카테고리들의 1D 배열을 담은 리스트 반환
    - 본 예제에서는 범주형 특성이 1개만 있으므로 배열 하나를 담은 리스트 반환
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/871124b5-f1d8-4235-8a2d-4a3313ad8974/image.png)
    
    - 가까이 있는 두 값이 떨어져 있는 두 값보다 더 비슷하다고만 생각하면 안 됨
    → 여기서는 1H OCEAN 과 NEAR OCEAN이 비슷함!!
    → 카테고리 0과 1보단 카테고리 0과 4가 가까움 !!
    → 카테고리 별 이진 특성 만들어서 해결함

- 카테고리 별 이진 특성 만들기 (원-핫 인코딩, One-Hot Encoding)
    - 새로운 특성 → 더미 특성
    - OneHotEncoder 클래스
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/55ae3197-e0b1-4ce1-a023-441f91835211/image.png)
        
        - 카테고리가 1H OCEAN일 때 한 특성이 1(핫)이고(그 외 특성은 0),
        카테고리가 INLAND일 때 다른 한 특성이 1이 되고(그 외 특성은 0)… 과 같은 방식으로 변환
        → 0이 아닌 원소의 위치만 기억함!!
        - 넘파이 배열이 아닌, **‘사이파이 희소행렬(SciPy Sparse Matrix)’**을 출력
        → (기억) 희소행렬은 0을 모두 메모리에 저장하지 않고, ‘**0이 아닌 원소의 위치만 저장**’함!!
        → 메모리 효율적임!!
        - OneHotEncoder 클래스 이전에 housing_cat_encoded 배열 (원래 배열)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/a32923a3-7580-4a84-8440-c6866b4196c6/image.png)
            
        - 첫 번째 특성 값
         : 원래 배열에서 인덱스 1인 특성만 가졌으므로, 인덱스 1만 1이고 나머지는 0이 되는데
          0이 아닌 원소의 위치인 인덱스 1만 기억함
        - 두 번째 특성 값
         : 원래 배열에서 인덱스 4인 특성만 가졌으므로, 인덱스 4만 1이고 나머지는 0이 되는데
          0이 아닌 원소의 위치인 인덱스 4만 기억함
        
    - toarray() 메서드
        - 희소행렬을 2차원 배열처럼 사용할 수는 있지만,
        toarray() 메서드를 이용하여 **(밀집된) 넘파이 배열로 변환**하여 볼 수 있음 (밀집 배열)
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/1b42f70e-7ca0-46dc-936b-722afd3e80a7/image.png)
            
        - toarray() 메서드로 변환된 배열에서도 인코더의 categories_ 인스턴스 변수를 사용해
        카테고리 리스트를 얻을 수 있음
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/665bbf25-6109-4dcb-8bbb-02a898a840e7/image.png)
            

- **카테고리 특성이 담을 수 있는 ‘카테고리 수가 많을 때’** (ex. 직업, 생물 종류 등)
    - **원-핫 인코딩은 많은 수의 입력 특성을 만들 수 있기 때문에 훈련을 느리게 하고 성능을 감소시킴**
    - 방안 1 : 범주형 입력값을 이 특성과 관련된 **숫자형 특성으로 바꾸는** 방법
        - 원-핫 인코딩처럼 범주형 특성 그대로 가져와 2차원 배열 만드는 것보다 ocean_proximity 같은 경우에 ‘**해안까지의 거리**’라는 숫자형 특성으로 바꿀 수 있음
    - 방안 2 : 각 카테고리를 임베딩(Embedding)이라고 부르는 학습 가능한 ‘**저차원 벡터**’로 바꾸는 방법
        - 훈련 동안에 각 카테고리의 표현을 학습함 (표현 학습의 하나의 예)
        - 13장, 17장 참조

### 나만의 변환기

- 특별한 정제 작업이나 어떤 특성들을 조합하는 등의 작업이 필요한데,
사이킷런에서 제공하지 않다면 ‘자신만의 변환기’ 만들어야 함
→ 자신만의 변환기를 파이프라인과 같은 사이킷런의 기능과 매끄럽게 연동하고 싶을 것

- 사이킷런에서 **덕 타이핑 (Duck typing)** 지원하므로
fit()(self를 반환), transform(), fit_transform() 메서드를 구현한 파이썬 클래스 만들면 됨
    - fit() 메서드
        - 아무 것도 하지 않고 self 리턴함 (즉, 아무런 값도 추정할 필요 없음)
        - 인자는 넘파이 배열이어야 함
    - transform() 메서드 : 속성 추가 기능 구현해야 함
        - np.c_() 함수: 두 어레이를 열을 축으로 해서 이어 붙이기
        - 인자는 넘파이 배열이어야 함
    - fit_transform() 메서드
        - TransformerMixin 클래스 상속하면 fit_transform() 메서드 자동 생성됨
- BaseEstimator 상속
    - BaseEstimator 상속하면 하이퍼파라미터 튜닝에 필요한 get_params(), set_params() 메서드 자동 생성됨
    - 단, 초기 설정 메서드(__init__())가 *args 또는 kargs 형식의 인자를 사용하지 않아야 함

- 조합 특성을 추가하는 나만의 변환기 예시
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/1ad2294b-b0c4-4032-9967-983f74e5cb8c/image.png)
    
    - 변환기가 add_bedrooms_per_room 하이퍼파라미터만 가짐
    (기본값=True / 다만 연습 용도니까 False로 해놓음)
    - **머신러닝 알고리즘에 도움이 될지 안 될지 모르는 하이퍼파라미터를 추가해보면서 확인할 수 있음
    → 이러한 자동화를 통해 더 많은 조합을 시도할 수 있고, 빠르게 최상의 조합을 찾을 가능성 높여줌**

### 특성 스케일링 (Feature Scaling)

- **입력 숫자 특성들의 스케일이 많이 다르면** 제대로 작동하지 않기 때문에 ‘**특성 스케일**링’ 필요
→ 모든 특성의 범위를 같도록 만들어줌
- **모든 변환기에서 스케일링은 전체 데이터가 아닌, 훈련 데이터에 대해서만 fit() 메서드를 적용해야 함
→ 이후 훈련 세트와 테스트 세트, 새로운 데이터에 대해 transform() 메서드를 사용함**
- 타깃값의 스케일링은 일반적으로 불필요 (본 예제에서는 중간 주택 가격임)

- min-max 스케일링 (정규화, Nomalization)
    - 0~1 범위에 들도록 값을 이동, 스케일 조정
    - 데이터에서 최솟값을 뺀 후, 최댓값과 최솟값의 차이로 나눔
    - 표준화보다 **이상치에 영향을 더 받음**
    - MinMaxScaler 변환기 : min-max 스케일링을 해줌
        - feature_range 매개변수를 이용해 0~1이 아닌 범위로 변경 가능
    - (주의) 사이킷런에서 입력데이터에서 각 행의 l_2 노름이 1이 되도록 조정하는 Nomalizer 전처리 기능 있는데, min-max 스케일링과 전혀 다른 결과 만듦 (정규화 여러 의미로 다양하게 사용됨)
        
        
- 표준화 (Standardization)
    - 평균 뺀 후 표준편차로 나눔
    - 평균이 0으로 되고, 분포의 분산이 1이 됨
    - 범위의 상한과 하한이 없어 어떤 알고리즘에서는 문제가 될 수 있음 (ex. 신경망)
    - min-max 스케일링보다 **이상치에 영향을 덜 받음**
    - StandardScaler 변환기 : 표준화를 해줌

### 변환 파이프라인 (더 깔끔하게 정리할 필요 있음!!)

- 변환 단계가 많아 정확한 순서대로 실행되게 하는 것이 중요함
- Pipeline 클래스
    - 사이킷런에서 제공하는 연속된 변환을 순서대로 처리할 수 있도록 클래스
    - 숫자 특성을 처리함
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/2ee36c4b-b914-45f7-94f5-3e1282bdab46/image.png)
    
    - 연속된 단계를 나타내는 변환기이름/추정기 쌍의 목록을 입력으로 받음
    - 마지막 단계에는 변환기와 추정기 모두 사용할 수 있지만 그 외에는 모두 변환기이어야 함
    (즉, fit_transform() 메서드를 가지고 있거나, fit()과 transform 메서드를 함께 가지고 있어야 함)
    - 이름은 이중 밑줄 __ 을 포함하지 않으면 무엇이든 됨
    - 하이퍼파라미터 튜닝할 때 필요할 것
    - 파이프라인의 fit() 메서드를 호출하면 (pipeline이름.fit() 호출)
    모든 변환기의 fit_transform() 메서드를 순서대로 호출하면서
    (fit_transform() 메서드가 없으면 fit()과 transform()을 차례로 호출함)
    한 단계의 출력을 다음 단계의 입력으로 전달함 (마지막 단계에서는 fit() 메서드만 호출함)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/845c45a4-1659-4ec7-a3db-45d24b75744b/image.png)
        
    - 파이프라인 객체는 마지막 추정기와 동일한 메서드를 제공함
    본 예제에서는 마지막 추정기가 변환기 StandardScaler임
    → 파이프라인이 데이터에 대해 모든 변환을 순서대로 적용하는 transform() 메서드를 가지고 있음 (앞에서 사용한 fit_transform() 메서드도 가지고 있음)

- 수치형 특성 파이프라인과 범주형 특성을 하나의 파이프라인으로 묶기
    - ColumnTransformer : 하나의 변환기로 각 열마다 적절한 변환을 적용해 모든 열을 처리할 수 있음
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/54fb9d31-be78-4d72-ac81-e6279fa2e519/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/40a18430-3694-459d-9cba-eef725067d03/image.png)
        
        - 주택 가격 데이터에 전체 변환 적용
        - 수치형 열 이름의 리스트와 범주형 열 이름의 리스트 생성
        - ColumnTransformer 클래스 객체 생성
            - 튜플의 리스트를 인자로 받음
            - 이름, 변환기, 변환기가 적용될 열이름(또는 인덱스)으로 이루어진 튜플의 리스트
            - 변환기를 사용하는 대신에 삭제하고 싶은 열이 있으면 ‘drop’ 문자열로 지정 가능
            - 변환기를 사용하는 대신에 변환을 적용하지 않을 열이 있다면 ‘passthrough’ 로 지정 가능
            (기본적으로 나열되지 않은 나머지 열은 삭제됨 → remainder 하이퍼파라미터에 어떤 변환기 또는 passthrough 를 지정할 수 있음)
            - 수치형 열은 num_pipeline을 사용해 변환되고, 범주형 열은 OneHotEncoder을 사용해 변환
        - 마지막에 ColumnTransform를 주택 데이터에 적용
        - 각 변환기를 적절한 열에 적용하고 그 결과를 두 번째 축을 따라 연결
        (변환기는 동일한 개수의 행을 반환해야 함)
        - OneHotEncoder는 희소 행렬을 반환하지만, num_pipeline은 밀집 행렬을 반환
            - 희소 행렬과 밀집 행렬 섞여 있을 때,
            ColumnTransformer는 최종 행렬의 밀집 정도를 추정함
            (즉 0이 아닌 원소의 비율)
            - 밀집도가 임곗값보다 낮으면 희소 행렬을 반환함
            (기본적으로 sparse_threshold = 0.3임)
            - 본 예제에서는 밀집 행렬이 반환됨
    - FeatureUnion 클래스 : 여러 변환기를 적용하고 결과를 합쳐줌
        - 각 변환기에 열을 따로 지정은 불가능하고 전체 데이터에 모두 적용됨

## 모델 선택과 훈련

- 현재 “문제 정의 → 데이터 읽기 → 탐색 → 훈련/테스트 세트 나눔
→ 머신러닝 알고리즘에 주입할 데이터 자동 정제, 준비하기 위한 변환 파이프라인 작성” 까지 함
- 머신러닝 모델 선택하고 훈련시켜야 함

### 훈련 세트에서 훈련하고 평가하기

- 선형 회귀 모델 훈련 (LinearRegresstion 클래스)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/ce438a35-8d38-4ad9-9284-b19d53da7d67/image.png)
    
    - fit() 메서드를 전처리된 훈련세트와 레이블을 인자로 사용하여 호출
    - **(주의)** LinearRegression 모델은 모델을 학습하는 대신에 무어-펜로즈 역행렬을 이용하여 직접 파라미터를 계산함 (4장 참조)
    - 훈련 세트에 있는 몇 개 샘플에 적용
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/8d78ce67-a881-4a06-9a5a-6e912fd08d50/image.png)
        
    - mean_square_error 함수
        - 이 회귀 모델의 RMSE(평균 제곱근 오차) 구하기
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/d0eff4df-08c1-4145-90f2-f6cd01aa281e/image.png)
            
        - 대부분 구역의 중간 주택 가격은 $120,000 ~ $265,000 사이인데,
        예측 오차가 $68633은 매우 만족스럽지 않은 결과임
        → 모델이 훈련 데이터에 과소적합된 사례임!!
        - 방안
            - 더 강력한 모델 선택 → 더 복잡한 모델을 시도해보자 (결정 트리 모델 이용!!)
            - 훈련 알고리즘에 더 좋은 특성을 주입 (로그 스케일된 인구 등)
            - 모델의 규제를 감소시킴 (현재는 모델에 규제를 적용하지 않았으니, 이는 제외)
            
- 결정 트리 (DecisionTreeRegressor)
    - 강력하고 복잡한 비선형 관계를 찾을 수 있음 (6장에서 설명)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/caa43c1c-1920-4ec2-ae27-bea46267b96a/image.png)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/48673a00-4930-4c85-9bcb-dbcfdcf5832b/image.png)
        
        - 오차가 0 나옴 → 너무 심한 과대적합된 것
        - 확신이 드는 모델이 론칭할 준비가 되기 전까지 테스트 세트는 사용하지 않으려 하므로
        훈련 세트의 일부분으로 훈련을 하고 다른 일부분은 모델 검증에 사용해야 함

### 교차 검증을 사용한 평가

- 결정 트리 모델 평가하는 방법
    - train_test_split 함수를 사용해 훈련 세트를 더 작은 훈련 세트와 검증 세트로 나누고,
    더 작은 훈련 세트에서 모델을 훈련시키고 검증 세트로 모델을 평가하는 방법
    → 너무 수고롭지만 어렵지 않고 매우 잘 작동
    
    - K-겹 교차 검증 (K-Fold Cross-Validation) 기능 사용하는 방법
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/f78cd701-d7d0-4827-b2dc-24f3763c256b/image.png)
        
        - 훈련 세트를 폴드(Fold)라 하는 10개의 서브셋으로 무작위 분할함
        - 결정 트리 모델을 10번 훈련하고 평가함
            - 이때, 매번 다른 폴드를 선택해 평가에 사용하고, 나머지 9개 폴드는 훈련에 사용
            - 10개의 평가 점수가 담긴 배열이 결과임
        - 평균 제곱 오차가 작을수록 좋은 비용 함수이므로 부호가 반대(-scores)가 되어야
        scoring 매개변수 정의에 맞음
        (회귀 모델에서 scoring 매개변수를 지정하지 않으면 기본적으로 0~1 사이의 값을 가지는 r2_score 사용됨)
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/8157a958-18d9-43d9-a913-25a973359164/image.png)
        
        - 교차 검증으로 모델의 성능을 추정하는 것뿐만 아니라 이 추정이 얼마나 정확한지 측정 가능
        - (결과) 결정 트리 모델이 과대적합되어 선형 회귀 모델보다 성능이 나쁘게 나옴!!
        
        - 사이킷런의 교차 검증 기능은 scoring 매개변수에 낮을 수록 좋은 비용 함수가 아니라 클수록 좋은 효용 함수를 기대함 → 평균 제곱오차(MSE)의 반댓값(음수)을 계산하는 neg_mean_squared_error 함수를 사용함
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/b36cf283-b29e-40ee-b405-bda42d100280/image.png)
            
        
    - RandomForestRegressor 모델 사용하는 방법 (7장에서 설명)
        - 랜덤 포레스트는 특성을 무작위로 선택해서 많은 결정 트리를 만들고 그 예측을 평균 내는 방식
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/12d4a187-2ef1-4382-bd1e-6970461a7f39/image.png)
            
        - 앙상블 학습
            - 여러 다른 모델을 모아서 하나의 모델을 만드는 것
            - 머신러닝 알고리즘의 성능을 극대화할 수 있음
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/76018e00-95f9-4a68-a383-8a9c45f61dcc/image.png)
        
        - 랜덤 포레스트로 교차 검증했을 때(50435), 선형회귀와 결정 트리 모델보다 성능이 좋게 나옴
        - BUT, 훈련 세트에 대한 점수가 검증 세트에 대한 점수보다 훨씬 낮으므로 이 모델도 훈련 세트에
        과대적합되어 있음 → 모델 간단히 하거나 제한을 하거나(규제), 더 많은 훈련 데이터 모음
    
    - 하이퍼파라미터 조정에 많은 시간 들이지 않은 2~5개 정도 모델 선정할 것임!
    (다양한 커널의 서포트 벡터 머신, 신경망 등)
    
    - 실험한 모델을 모두 저장해두면 쉽게 모델을 복원 가능
        - 교차 검증 점수와 실제 예측값, 하이퍼파라미터와 훈련된 모델 파라미터 모두 저장해야 함
        - 여러 모델의 점수와 모델인 만든 오차 쉽게 비교 가능
        - 파이썬 pickle 패키지, joblib 라이브러리(큰 넘파이 배열 저장 효율적임)를 사용하여
        사이킷런 모델 저장 가능

## 모델 세부 튜닝

- 가능성 있는 모델들을 추렸다고 가정
- 이 모델들을 세부 튜닝해야 함
- 그리드 탐색, 랜덤 탐색, 앙상블 학습으로 각각의 모델의 최고의 성능을 보여주는 하이퍼파라미터 찾음

### 그리드 탐색

- 만족할 만한 하이퍼파라미터 조합을 찾을 때까지 하이퍼파라미터 조정
    - 많은 경우의 수를 탐색하기에 시간이 부족
- GridSearchCV 이용
    - 탐색하고자 하는 하이퍼파라미터와 시도해볼 값을 지정하면 가능한 모든 하이퍼파라미터 조합에
    대해 교차 검증을 사용해 평가하게 됨
    - RandomForestRegressor에 대한 최적의 하이퍼파라미터 조합 탐색
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/2a5448cb-4d6f-437a-bd04-7d3c7528a170/image.png)
        
        - param_grid
            - 첫 번째 dict에 있는 n_estimators와 max_feature 하이퍼파라미터 조합인
            12(3*4)개 평가
            - 두 번째 dict에 있는 하이퍼파라미터 조합인 6(2*3)개 시도함
            (bootstrap 하이퍼파라미터 : False 설정 / 기본값은 True임)
        - 모두 합하여 그리드 탐색이 RandomForestRegressor 하이퍼파라미터 값의
        12+6=18개 조합 탐색, 각각 다섯 번 모델을 훈련시킴(5-겹 교차 검증 사용)
        → 전체 훈련 횟수는 18*5=90임
        - GridSearchCV가 기본값인 refit=True로 초기화되었다면
        교차 검증으로 최적의 추정기를 찾은 다음 전체 훈련 세트로 다시 훈련시킴
        → 일반적으로 데이터가 많을수록 성능이 향상되므로 좋은 방법임
        
    - 최적의 조합 확인
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/5f921fac-413e-4051-ad1b-02fff28c7a47/image.png)
        
        → max_features 하이퍼파라미터가 8, n_estimators 하이퍼파라미터가 30일 때 최적의 솔루션
        
    
    - 최적의 추정기(최상의 랜덤 포레스트 회귀 모델)를 best_estimator에 저장
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/0cf3ab9c-6d2f-495c-9b14-c06468a0a3db/image.png)
        
    - RMSE 점수 : 49898.98
    → 기본 하이퍼파라미터 설정으로 아까 얻은(랜덤포레스트 교차 검증) 50435 보다 조금 더 좋음
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/a5b0bf6c-2795-454a-93cc-f730cc6538ce/image.png)
        
- 데이터 준비 단계를 하나의 하이퍼파라미터처럼 다룰 수 있음
 ex. 그리드 탐색이 확실하지 않은 특성을 추가할지 말지 자동으로 정할 수 있음
 (CombinedAttributesAdder 변환기의 add_bedrooms_per_room 하이퍼파라미터를 사용하여
  특성을 추가할지 결정함)
- 비슷하게 이상치나 값이 빈 특성을 다루거나 특성 선택 등을 자동으로 처리하는 데 그리드 탐색 사용함

### 랜덤 탐색

- 그리드 탐색 방법은 비교적 적은 수의 조합을 탐구할 때 괜찮음
- 하이퍼파라미터 탐색 공간이 커지면 랜덤 탐색 RandomizedSearchCV 사용하는 것이 좋음
- RandomizedSearchCV는 GridSearchCV와 비슷하지만, 가능한 모든 조합을 시도하는 대신
각 반복마다 하이퍼파라미터에 임의의 수를 대입하여 지정한 횟수만큼 평가함
- 랜덤 탐색을 1,000회 반복하도록 실행하면 하이퍼파라미터마다 각기 다른 1,000개의 값을 탐색함
(그리드 탐색에서는 하이퍼파라미터마다 몇 개의 값만 탐색함)
- 단순히 반복 횟수를 조절하는 것만으로 하이퍼파라미터 탐색에 투입할 컴퓨팅 자원 제어 가능
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/4af8f7db-e152-424f-aac1-b7c696064149/image.png)
    

### 앙상블 방법

- 모델의 그룹(앙상블)이 최상의 단일 모델보다 더 나은 성능을 발휘할 때가 많음
- 결정 트리의 앙상블인 랜덤 포레스트가 결정 트리 하나보다 성능이 더 좋음

### 최상의 모델과 오차 분석

- 최상의 모델을 분석하면 문제에 대한 좋은 통찰을 얻을 수 있음
- RandomForestRegressor가 정확한 예측을 만들기 위한 각 특성의 상대적인 중요도 알려줌
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/0e889293-9966-43f7-81d4-055143b86f83/image.png)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/e0380880-7241-44af-bb0c-ddfd534f23b1/image.png)
    
    → 덜 중요한 특성들을 제외할 수 있음
        (ocean_proximity 카테고리 중 하나만 실제로 유용하므로 다른 카테고리 제외할 수 있음)
    

- 시스템이 특정한 오차를 만들었으면 문제의 원인을 이해하고, 추가 특성을 포함시키거나, 불필요한 특성을 제거하거나, 이상치를 제거하는 등 문제를 해결하는 방법을 찾아야 함

### 테스트 세트로 시스템 평가하기

- 모델 튜닝하여 어느정도 만족할 만한 모델을 얻음
- 테스트 세트로 최종 모델 평가해야 함
- 테스트 세트에 예측 변수와 레이블을 얻은 후 full_pipeline 사용해 데이터 변환하여 평가
(테스트 세트에서 훈련하면 안 되므로 fit_transform()이 아닌, transform()을 호출해야 함!!)
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/f7554d5f-823a-4085-bfb0-17a8fd20ea85/image.png)
    
    → 일반화 오차의 추정이 론칭을 결정하기에 충분하지 않을 수 있음
    → 이 추정값이 얼마나 정확한지 알기 위해 scipy.stats.t.interval() 사용해
        일반화 오차의 95% 신뢰구간을 계산할 수 있음
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/16f3c269-9300-4f59-8c4d-30e2ea992a04/image.png)
    
    → 하이퍼파라미터 튜닝 많이 하면 교차 검증을 사용해 측정한 것보다 조금 성능이 낮은 것이 보통임
    (테스트 세트에서 성능 수치 좋게 하려고 하이퍼파라미터 튜닝하면 새로운 데이터에 일반화되기 어려움)
    

## 론칭, 모니터링, 시스템 유지 보수

- 제품 시스템에 적용하기 위한 준비를 함 (코드 정리, 문서와 테스트 케이스를 작성함)
- 모델을 상용 환경에 배포할 수 있음
    - 전체 전처리 파이프라인과 예측 파이프라인이 포함된 훈련된 사이킷런 모델을 저장함(ex. joblib 이용)
    - 훈련된 모델을 상용환경에서 로드하고 predict() 메서드를 호출해 예측을 만듦
    - 예시
        
        ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/798c8520-9e5d-4ead-adaf-e25b7ac4deb5/ea50ba67-1022-412d-99eb-3cf7bf4a6ad4/image.png)
        
        - 모델이 웹사이트 안에서 사용될 수 있음
        - 사용자가 새로운 구역에 관한 정보를 입력하고 ‘가격 예측하기’ 버튼 누름
        - 이 데이터를 포함한 쿼리가 웹서버로 전송되어 웹 애플리케이션으로 전달됨
            - 이 애플리케이션의 코드가 모델의 predict() 메서드를 호출함
            (모델을 사용할 때마다 로드하지 않고 서버가 시작할 때 모델을 로드하는 것이 좋음)
        - 또는 웹 애플리케이션이 REST API를 통해 질의할 수 있는 전용 웹 서비스로 모델을 감쌈
            - 주 애플리케이션을 건드리지 않고 모델을 새 버전으로 업그레이드하기 쉬움
            - 필요한 만큼 웹 서비스를 시작하고 웹 애플리케이션에서 웹 서비스로 오는 요청을 로드 밸런싱할 수 있기 때문에 규모 확장 쉬움
            - 또한, 웹 애플리케이션을 파이썬이 아닌 다른 어떤 언어로도 작성 가능함
        - 또는 구글 클라우드 AI 플랫폼과 같은 클라우드에 배포함
            - 모델을 저장하고 구글 클라우드 스토리지(GCS)에 업로드
            - 구글 클라우드 AI 플랫폼으로 이동하여 새로운 모델 버전을 만들고 GCS 파일 지정gka
            - 로드 밸런싱과 자동 확장을 처리하는 간단한 웹 서비스를 만들어 줌
            - 입력 데이터 (ex. 구역 정보)를 담은 JSON 요청을 받고 예측을 담은 JSON 응답을 반환
        - 웹사이트에서 (or 현재 사용되는 어떤 상용 환경에서) 웹 서비스 사용 가능
    
    - 시스템의 실시간 성능 체크하고 성능이 떨어졌을 때 알람 통지하는 모니터링 코드 작성
        - 시스템의 고장난 컴포넌트 때문에 일어날 것 같은 갑작스러운 성능 감소 감지
        - 긴 시간 동안 눈에 띄지 않게 성능이 서서히 감소하는 상황 감지

122쪽…
